{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",0)\n",
    "print(f\"Device:\\t\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50844e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_list=['BRNT','BRID','BRIL','BRLC','BRDC']\n",
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':1000,\n",
    "        'n_classes':5,\n",
    "        'data_path':'../../../data/NIPA/',\n",
    "        'inch':3,\n",
    "        }\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# ÏÉàÎ°úÏö¥ ÏÖÄÏóê Ï∂îÍ∞ÄÌï¥ÏÑú ÌÖåÏä§Ìä∏Ìï¥Î≥¥ÏÑ∏Ïöî\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. BatchNorm ÏÉÅÌÉú ÌôïÏù∏\n",
    "def check_batchnorm_stats(model):\n",
    "    \"\"\"BatchNorm Î†àÏù¥Ïñ¥Ïùò running_meanÍ≥º running_var ÌôïÏù∏\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            if torch.isnan(module.running_mean).any() or torch.isnan(module.running_var).any():\n",
    "                print(f\"NaN detected in {name}\")\n",
    "                print(f\"Running mean has NaN: {torch.isnan(module.running_mean).any()}\")\n",
    "                print(f\"Running var has NaN: {torch.isnan(module.running_var).any()}\")\n",
    "\n",
    "# 2. BatchNorm Ï¥àÍ∏∞Ìôî Ìï®Ïàò\n",
    "def reset_batchnorm_stats(model):\n",
    "    \"\"\"BatchNormÏùò running statistics Ï¥àÍ∏∞Ìôî\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.reset_running_stats()\n",
    "\n",
    "# 3. ÏïàÏ†ÑÌïú eval Î™®Îìú ÏÑ§Ï†ï\n",
    "def safe_eval_mode(model):\n",
    "    \"\"\"ÏïàÏ†ÑÌïòÍ≤å eval Î™®ÎìúÎ°ú Ï†ÑÌôò\"\"\"\n",
    "    model.eval()\n",
    "    # BatchNorm Î†àÏù¥Ïñ¥Ïùò momentumÏùÑ ÏõêÎûòÎåÄÎ°ú Î≥µÍµ¨\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d) and hasattr(module, 'backup_momentum'):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list=glob(params['data_path']+class_list[i]+'/*.jpeg')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "        \n",
    "train_images=torch.zeros((len(image_path),params['inch'],params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(len(image_path))):\n",
    "    train_images[i]=trans(Image.open(image_path[i]).convert('RGB').resize((params['image_size'],params['image_size'])))\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_images, image_label, test_size=0.2, random_state=42)\n",
    "# train_dataset=CustomDataset(params,X_train,F.one_hot(torch.tensor(y_train)).to(torch.int64))\n",
    "train_dataset=CustomDataset(params,train_images,F.one_hot(torch.tensor(image_label)).to(torch.int64))\n",
    "val_dataset=CustomDataset(params,X_test,F.one_hot(torch.tensor(y_test)).to(torch.int64))\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('tf_efficientnetv2_xl', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(custom_model, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size, 2048, 1, 1)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n",
    "        \n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "            \n",
    "import transformers\n",
    "\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = custom_model(len(class_list),1280,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "base_optimizer = torch.optim.AdamW\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=params['lr'])\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_list)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6038747",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "sig=nn.Sigmoid()\n",
    "model_path='../../../model/NIPA_classification/Breast/'\n",
    "create_dir(model_path)\n",
    "val_acc_list=[]\n",
    "for epoch in range(1000):\n",
    "    train=tqdm(dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        \n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        enable_running_stats(model)\n",
    "        optimizer.zero_grad()  # optimizer zero Î°ú Ï¥àÍ∏∞Ìôî\n",
    "        predict = model(x).to(device)\n",
    "        cost = F.cross_entropy(predict, y) # cost Íµ¨Ìï®\n",
    "        cost.backward() # costÏóê ÎåÄÌïú backward Íµ¨Ìï®\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        disable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost1 = F.cross_entropy(predict, y) # cost Íµ¨Ìï®\n",
    "        cost1.backward() # costÏóê ÎåÄÌïú backward Íµ¨Ìï®\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        running_loss += cost.item()\n",
    "\n",
    "        train.set_description(f\"epoch: {epoch+1}/{1000} Step: {count+1} loss : {running_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "#validation\n",
    "    val=tqdm(val_dataloader)\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = F.cross_entropy(predict, y) # cost Íµ¨Ìï®\n",
    "            acc=accuracy(predict.argmax(dim=1),y.argmax(dim=1))\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            val.set_description(f\"Validation epoch: {epoch+1}/{1000} Step: {count+1} loss : {val_running_loss/count:.4f}  accuracy: {acc_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "    if epoch%100==5:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1) \n",
    "        plt.title('loss_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "        plt.plot(np.arange(epoch+1),val_loss_list,label='validation_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)  \n",
    "        plt.title('acc_graph')\n",
    "        plt.plot(np.arange(epoch+1),val_acc_list,label='validation_acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), f'{model_path}modelEff_v2_XL_SAM_'+str(epoch)+'.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "torch.save(model.state_dict(), f'{model_path}modelEff_v2_XL_SAM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Performance Evaluation\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_path='../../../model/NIPA_classification/Breast/'\n",
    "matplotlib.rcParams['font.size'] = 10\n",
    "\n",
    "# Load the best model\n",
    "# best_model_path = f'{model_path}modelEff_v2_XL_SAM_13.pt'\n",
    "best_model_path = f'{model_path}modelEff_v2_XL_SAM_127.pt'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.train()  # Set to evaluation mode\n",
    "\n",
    "# Test set evaluation\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_probabilities = []\n",
    "\n",
    "print(\"Evaluating on Test Set...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(val_dataloader, desc=\"Testing\"):\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        outputs = model(x)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(torch.argmax(y, dim=1).cpu().numpy())\n",
    "        test_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "test_probabilities = np.array(test_probabilities)\n",
    "\n",
    "# Calculate basic metrics\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(test_labels, test_predictions, average=None, zero_division=0)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='macro', zero_division=0)\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "# Get confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "print(\"\\nüìä Basic Test Set Performance Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Macro Average - Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Weighted Average - Precision: {weighted_precision:.4f}, Recall: {weighted_recall:.4f}, F1-Score: {weighted_f1:.4f}\")\n",
    "\n",
    "print(\"\\nüîÑ Confusion Matrix:\")\n",
    "print(\"-\"*30)\n",
    "cm_df = pd.DataFrame(cm, index=class_list, columns=class_list)\n",
    "print(cm_df.to_string())\n",
    "\n",
    "# Calculate per-class metrics with confidence intervals\n",
    "def calculate_wilson_ci(successes, trials, confidence=0.95):\n",
    "    \"\"\"Calculate Wilson confidence interval for binomial proportion\"\"\"\n",
    "    if trials == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    p = successes / trials\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    \n",
    "    denominator = 1 + z**2 / trials\n",
    "    centre = (p + z**2 / (2 * trials)) / denominator\n",
    "    delta = z * np.sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denominator\n",
    "    \n",
    "    ci_lower = max(0, centre - delta)\n",
    "    ci_upper = min(1, centre + delta)\n",
    "    \n",
    "    return p, ci_lower, ci_upper\n",
    "\n",
    "# Calculate per-class metrics\n",
    "per_class_results = []\n",
    "for i in range(len(class_list)):\n",
    "    # For class i, calculate TP, FP, TN, FN\n",
    "    tp = cm[i, i]  # True positives\n",
    "    fp = np.sum(cm[:, i]) - tp  # False positives (predicted as class i but actually other classes)\n",
    "    fn = np.sum(cm[i, :]) - tp  # False negatives (actually class i but predicted as other classes)\n",
    "    tn = np.sum(cm) - tp - fp - fn  # True negatives\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity_val = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1_val = 2 * precision_val * recall_val / (precision_val + recall_val) if (precision_val + recall_val) > 0 else 0\n",
    "    \n",
    "    # Binary accuracy for this class vs all others\n",
    "    binary_accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0\n",
    "    \n",
    "    # Calculate confidence intervals using Wilson method\n",
    "    precision_ci = calculate_wilson_ci(tp, tp + fp)\n",
    "    recall_ci = calculate_wilson_ci(tp, tp + fn)\n",
    "    specificity_ci = calculate_wilson_ci(tn, tn + fp)\n",
    "    accuracy_ci = calculate_wilson_ci(tp + tn, tp + fp + tn + fn)\n",
    "    \n",
    "    # F1 CI using bootstrap approximation\n",
    "    if f1_val > 0 and (tp + fn) > 0:\n",
    "        se_f1 = np.sqrt(f1_val * (1 - f1_val) / (tp + fn))\n",
    "        z = stats.norm.ppf(0.975)\n",
    "        f1_ci_lower = max(0, f1_val - z * se_f1)\n",
    "        f1_ci_upper = min(1, f1_val + z * se_f1)\n",
    "    else:\n",
    "        f1_ci_lower = f1_ci_upper = 0\n",
    "    \n",
    "    per_class_results.append({\n",
    "        'Class': class_list[i],\n",
    "        'TP': int(tp),\n",
    "        'FP': int(fp),\n",
    "        'TN': int(tn),\n",
    "        'FN': int(fn),\n",
    "        'Support': int(support[i]),\n",
    "        'Accuracy': binary_accuracy,\n",
    "        'Accuracy_CI_Lower': accuracy_ci[1],\n",
    "        'Accuracy_CI_Upper': accuracy_ci[2],\n",
    "        'Precision': precision_val,\n",
    "        'Precision_CI_Lower': precision_ci[1],\n",
    "        'Precision_CI_Upper': precision_ci[2],\n",
    "        'Recall': recall_val,\n",
    "        'Recall_CI_Lower': recall_ci[1],\n",
    "        'Recall_CI_Upper': recall_ci[2],\n",
    "        'Specificity': specificity_val,\n",
    "        'Specificity_CI_Lower': specificity_ci[1],\n",
    "        'Specificity_CI_Upper': specificity_ci[2],\n",
    "        'F1_Score': f1_val,\n",
    "        'F1_Score_CI_Lower': f1_ci_lower,\n",
    "        'F1_Score_CI_Upper': f1_ci_upper\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(per_class_results)\n",
    "\n",
    "# Calculate AUC for multiclass (One-vs-Rest approach)\n",
    "n_classes = len(class_list)\n",
    "roc_auc = {}\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "for i in range(n_classes):\n",
    "    # Create binary labels for class i vs rest\n",
    "    y_binary = (test_labels == i).astype(int)\n",
    "    y_scores = test_probabilities[:, i]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_binary, y_scores)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Calculate macro-average ROC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Calculate micro-average ROC\n",
    "y_test_binary = label_binarize(test_labels, classes=range(n_classes))\n",
    "if n_classes == 2:\n",
    "    y_test_binary = np.hstack((1-y_test_binary, y_test_binary))\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binary.ravel(), test_probabilities.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Add AUC to results\n",
    "for i, row in results_df.iterrows():\n",
    "    results_df.loc[i, 'AUC'] = roc_auc[i]\n",
    "\n",
    "print(f\"\\nMacro AUC: {roc_auc['macro']:.4f}, Micro AUC: {roc_auc['micro']:.4f}\")\n",
    "\n",
    "print(\"\\nüìã Per-Class Performance with 95% Confidence Intervals:\")\n",
    "print(\"-\"*160)\n",
    "print(f\"{'Class':>6} | {'Acc':>8} {'[CI]':>12} | {'Prec':>8} {'[CI]':>12} | {'Recall':>8} {'[CI]':>12} | {'Spec':>8} {'[CI]':>12} | {'F1':>8} {'[CI]':>12} | {'AUC':>6} | {'Supp':>4}\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['Class']:>6} | \"\n",
    "          f\"{row['Accuracy']:.3f} \"\n",
    "          f\"(¬±{row['Accuracy_CI_Upper']-row['Accuracy']:.3f}) | \"\n",
    "          f\"{row['Precision']:.3f} \"\n",
    "          f\"(¬±{row['Precision_CI_Upper']-row['Precision']:.3f}) | \"\n",
    "          f\"{row['Recall']:.3f} \"\n",
    "          f\"(¬±{row['Recall_CI_Upper']-row['Recall']:.3f}) | \"\n",
    "          f\"{row['Specificity']:.3f} \"\n",
    "          f\"(¬±{row['Specificity_CI_Upper']-row['Specificity']:.3f}) | \"\n",
    "          f\"{row['F1_Score']:.3f} \"\n",
    "          f\"(¬±{row['F1_Score_CI_Upper']-row['F1_Score']:.3f}) | \"\n",
    "          f\"{row['AUC']:.3f} | \"\n",
    "          f\"{row['Support']:>4}\")\n",
    "\n",
    "# Add separator line\n",
    "print(\"-\"*160)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = results_df['Accuracy'].mean()\n",
    "avg_specificity = results_df['Specificity'].mean()\n",
    "\n",
    "# Calculate weighted specificity\n",
    "weighted_specificity = np.average(results_df['Specificity'], weights=support)\n",
    "\n",
    "# Add Macro Average row\n",
    "print(f\"{'Macro':>6} | \"\n",
    "      f\"{avg_accuracy:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{macro_precision:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{macro_recall:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{avg_specificity:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{macro_f1:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{roc_auc['macro']:.3f} | \"\n",
    "      f\"{sum(support):>4}\")\n",
    "\n",
    "# Add Weighted Average row\n",
    "print(f\"{'Weight':>6} | \"\n",
    "      f\"{accuracy:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_precision:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_recall:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_specificity:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_f1:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{roc_auc['micro']:.3f} | \"\n",
    "      f\"{sum(support):>4}\")\n",
    "\n",
    "# Validation: Check if metrics make sense\n",
    "print(\"\\nüîç Validation Check:\")\n",
    "print(\"-\"*50)\n",
    "for _, row in results_df.iterrows():\n",
    "    # Check if F1 is reasonable given precision and recall\n",
    "    expected_f1 = 2 * row['Precision'] * row['Recall'] / (row['Precision'] + row['Recall']) if (row['Precision'] + row['Recall']) > 0 else 0\n",
    "    f1_diff = abs(row['F1_Score'] - expected_f1)\n",
    "    \n",
    "    print(f\"{row['Class']:>6}: TP={row['TP']:>3}, FP={row['FP']:>3}, TN={row['TN']:>3}, FN={row['FN']:>3}\")\n",
    "    print(f\"        Expected F1: {expected_f1:.4f}, Actual F1: {row['F1_Score']:.4f}, Diff: {f1_diff:.4f}\")\n",
    "    \n",
    "    # Check sklearn vs manual calculation\n",
    "    sklearn_precision = precision[_] if _ < len(precision) else 0\n",
    "    sklearn_recall = recall[_] if _ < len(recall) else 0\n",
    "    sklearn_f1 = f1[_] if _ < len(f1) else 0\n",
    "    \n",
    "    print(f\"        Sklearn - Prec: {sklearn_precision:.4f}, Recall: {sklearn_recall:.4f}, F1: {sklearn_f1:.4f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Performance metrics bar chart\n",
    "ax1 = axes[0]\n",
    "x_pos = np.arange(len(class_list))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1_Score']\n",
    "colors = ['skyblue', 'orange', 'lightgreen', 'salmon', 'gold']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    values = results_df[metric].values\n",
    "    ci_lower = results_df[f'{metric}_CI_Lower'].values\n",
    "    ci_upper = results_df[f'{metric}_CI_Upper'].values\n",
    "    errors = [values - ci_lower, ci_upper - values]\n",
    "    \n",
    "    bars = ax1.bar(x_pos + i*width - 2*width, values, width, \n",
    "                   yerr=errors, capsize=3, label=metric, \n",
    "                   alpha=0.8, color=color)\n",
    "\n",
    "ax1.set_xlabel('Classes')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Per-Class Performance Metrics with 95% CI')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(class_list)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: Confusion Matrix\n",
    "ax2 = axes[1]\n",
    "im = ax2.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "ax2.set_title('Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(class_list)):\n",
    "    for j in range(len(class_list)):\n",
    "        text = ax2.text(j, i, str(cm[i, j]),\n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "ax2.set_xticks(range(len(class_list)))\n",
    "ax2.set_yticks(range(len(class_list)))\n",
    "ax2.set_xticklabels(class_list)\n",
    "ax2.set_yticklabels(class_list)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary for paper\n",
    "print(\"\\nüìù Summary for Paper:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"The breast cancer classification model achieved an overall accuracy of {accuracy:.4f} on the test set.\")\n",
    "print(f\"The macro-averaged F1-score was {macro_f1:.4f}, indicating {['poor', 'fair', 'good', 'excellent'][min(3, int(macro_f1*4))]} performance across all classes.\")\n",
    "print(f\"The macro-averaged AUC was {roc_auc['macro']:.4f}, demonstrating {['poor', 'fair', 'good', 'excellent'][min(3, int(roc_auc['macro']*4))]} discriminative ability.\")\n",
    "\n",
    "# Individual class performance\n",
    "best_f1_idx = results_df['F1_Score'].idxmax()\n",
    "worst_f1_idx = results_df['F1_Score'].idxmin()\n",
    "print(f\"Best performing class: {results_df.loc[best_f1_idx, 'Class']} (F1-Score: {results_df.loc[best_f1_idx, 'F1_Score']:.4f}, AUC: {results_df.loc[best_f1_idx, 'AUC']:.4f})\")\n",
    "print(f\"Most challenging class: {results_df.loc[worst_f1_idx, 'Class']} (F1-Score: {results_df.loc[worst_f1_idx, 'F1_Score']:.4f}, AUC: {results_df.loc[worst_f1_idx, 'AUC']:.4f})\")\n",
    "\n",
    "# Export results\n",
    "results_summary = {\n",
    "    'Metric': ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1-Score', \n",
    "               'Weighted Precision', 'Weighted Recall', 'Weighted F1-Score', \n",
    "               'Macro AUC', 'Micro AUC'],\n",
    "    'Value': [accuracy, macro_precision, macro_recall, macro_f1, \n",
    "              weighted_precision, weighted_recall, weighted_f1, \n",
    "              roc_auc['macro'], roc_auc['micro']]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "results_df.to_csv(f'{model_path}detailed_per_class_results.csv', index=False)\n",
    "summary_df.to_csv(f'{model_path}overall_results.csv', index=False)\n",
    "cm_df.to_csv(f'{model_path}confusion_matrix.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
