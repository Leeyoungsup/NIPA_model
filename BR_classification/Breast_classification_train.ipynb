{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",0)\n",
    "print(f\"Device:\\t\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50844e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_list=['BRNT','BRID','BRIL','BRLC','BRDC']\n",
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':1000,\n",
    "        'n_classes':5,\n",
    "        'data_path':'../../../data/NIPA/',\n",
    "        'inch':3,\n",
    "        }\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# 새로운 셀에 추가해서 테스트해보세요\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. BatchNorm 상태 확인\n",
    "def check_batchnorm_stats(model):\n",
    "    \"\"\"BatchNorm 레이어의 running_mean과 running_var 확인\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            if torch.isnan(module.running_mean).any() or torch.isnan(module.running_var).any():\n",
    "                print(f\"NaN detected in {name}\")\n",
    "                print(f\"Running mean has NaN: {torch.isnan(module.running_mean).any()}\")\n",
    "                print(f\"Running var has NaN: {torch.isnan(module.running_var).any()}\")\n",
    "\n",
    "# 2. BatchNorm 초기화 함수\n",
    "def reset_batchnorm_stats(model):\n",
    "    \"\"\"BatchNorm의 running statistics 초기화\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.reset_running_stats()\n",
    "\n",
    "# 3. 안전한 eval 모드 설정\n",
    "def safe_eval_mode(model):\n",
    "    \"\"\"안전하게 eval 모드로 전환\"\"\"\n",
    "    model.eval()\n",
    "    # BatchNorm 레이어의 momentum을 원래대로 복구\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d) and hasattr(module, 'backup_momentum'):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "def transback(data:Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self,parmas, images,label):\n",
    "        \n",
    "        self.images = images\n",
    "        self.args=parmas\n",
    "        self.label=label\n",
    "        \n",
    "    def trans(self,image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image=self.images[index]\n",
    "        label=self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label=[]\n",
    "image_path=[]\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list=glob(params['data_path']+class_list[i]+'/*.jpeg')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "        \n",
    "train_images=torch.zeros((len(image_path),params['inch'],params['image_size'],params['image_size']))\n",
    "for i in tqdm(range(len(image_path))):\n",
    "    train_images[i]=trans(Image.open(image_path[i]).convert('RGB').resize((params['image_size'],params['image_size'])))\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_images, image_label, test_size=0.2, random_state=42)\n",
    "# train_dataset=CustomDataset(params,X_train,F.one_hot(torch.tensor(y_train)).to(torch.int64))\n",
    "train_dataset=CustomDataset(params,train_images,F.one_hot(torch.tensor(image_label)).to(torch.int64))\n",
    "val_dataset=CustomDataset(params,X_test,F.one_hot(torch.tensor(y_test)).to(torch.int64))\n",
    "dataloader=DataLoader(train_dataset,batch_size=params['batch_size'],shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('tf_efficientnetv2_xl', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(custom_model, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size, 2048, 1, 1)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n",
    "        \n",
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, _BatchNorm):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "            \n",
    "import transformers\n",
    "\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = custom_model(len(class_list),1280,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "base_optimizer = torch.optim.AdamW\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=params['lr'])\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_list)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6038747",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "sig=nn.Sigmoid()\n",
    "model_path='../../../model/NIPA_classification/Breast/'\n",
    "create_dir(model_path)\n",
    "val_acc_list=[]\n",
    "for epoch in range(1000):\n",
    "    train=tqdm(dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    model.train()\n",
    "    for x, y in train:\n",
    "        \n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        enable_running_stats(model)\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).to(device)\n",
    "        cost = F.cross_entropy(predict, y) # cost 구함\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        disable_running_stats(model)\n",
    "        predict = model(x).to(device)\n",
    "        cost1 = F.cross_entropy(predict, y) # cost 구함\n",
    "        cost1.backward() # cost에 대한 backward 구함\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        running_loss += cost.item()\n",
    "\n",
    "        train.set_description(f\"epoch: {epoch+1}/{1000} Step: {count+1} loss : {running_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "#validation\n",
    "    val=tqdm(val_dataloader)\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            predict = model(x).to(device)\n",
    "            cost = F.cross_entropy(predict, y) # cost 구함\n",
    "            acc=accuracy(predict.argmax(dim=1),y.argmax(dim=1))\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            val.set_description(f\"Validation epoch: {epoch+1}/{1000} Step: {count+1} loss : {val_running_loss/count:.4f}  accuracy: {acc_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "    if epoch%100==5:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1) \n",
    "        plt.title('loss_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "        plt.plot(np.arange(epoch+1),val_loss_list,label='validation_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)  \n",
    "        plt.title('acc_graph')\n",
    "        plt.plot(np.arange(epoch+1),val_acc_list,label='validation_acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), f'{model_path}modelEff_v2_XL_SAM_'+str(epoch)+'.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "torch.save(model.state_dict(), f'{model_path}modelEff_v2_XL_SAM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798615f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Performance Evaluation\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_path='../../../model/NIPA_classification/Breast/'\n",
    "matplotlib.rcParams['font.size'] = 10\n",
    "\n",
    "# Load the best model\n",
    "# best_model_path = f'{model_path}modelEff_v2_XL_SAM_13.pt'\n",
    "best_model_path = f'{model_path}modelEff_v2_XL_SAM_127.pt'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.train()  # Set to evaluation mode\n",
    "\n",
    "# Test set evaluation\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_probabilities = []\n",
    "\n",
    "print(\"Evaluating on Test Set...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(val_dataloader, desc=\"Testing\"):\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        outputs = model(x)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(torch.argmax(y, dim=1).cpu().numpy())\n",
    "        test_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "test_probabilities = np.array(test_probabilities)\n",
    "\n",
    "# Calculate basic metrics\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(test_labels, test_predictions, average=None, zero_division=0)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='macro', zero_division=0)\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "# Get confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "print(\"\\n📊 Basic Test Set Performance Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Macro Average - Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Weighted Average - Precision: {weighted_precision:.4f}, Recall: {weighted_recall:.4f}, F1-Score: {weighted_f1:.4f}\")\n",
    "\n",
    "print(\"\\n🔄 Confusion Matrix:\")\n",
    "print(\"-\"*30)\n",
    "cm_df = pd.DataFrame(cm, index=class_list, columns=class_list)\n",
    "print(cm_df.to_string())\n",
    "\n",
    "# Calculate per-class metrics with confidence intervals\n",
    "def calculate_wilson_ci(successes, trials, confidence=0.95):\n",
    "    \"\"\"Calculate Wilson confidence interval for binomial proportion\"\"\"\n",
    "    if trials == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    p = successes / trials\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    \n",
    "    denominator = 1 + z**2 / trials\n",
    "    centre = (p + z**2 / (2 * trials)) / denominator\n",
    "    delta = z * np.sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denominator\n",
    "    \n",
    "    ci_lower = max(0, centre - delta)\n",
    "    ci_upper = min(1, centre + delta)\n",
    "    \n",
    "    return p, ci_lower, ci_upper\n",
    "\n",
    "# Calculate per-class metrics\n",
    "per_class_results = []\n",
    "for i in range(len(class_list)):\n",
    "    # For class i, calculate TP, FP, TN, FN\n",
    "    tp = cm[i, i]  # True positives\n",
    "    fp = np.sum(cm[:, i]) - tp  # False positives (predicted as class i but actually other classes)\n",
    "    fn = np.sum(cm[i, :]) - tp  # False negatives (actually class i but predicted as other classes)\n",
    "    tn = np.sum(cm) - tp - fp - fn  # True negatives\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity_val = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1_val = 2 * precision_val * recall_val / (precision_val + recall_val) if (precision_val + recall_val) > 0 else 0\n",
    "    \n",
    "    # Binary accuracy for this class vs all others\n",
    "    binary_accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0\n",
    "    \n",
    "    # Calculate confidence intervals using Wilson method\n",
    "    precision_ci = calculate_wilson_ci(tp, tp + fp)\n",
    "    recall_ci = calculate_wilson_ci(tp, tp + fn)\n",
    "    specificity_ci = calculate_wilson_ci(tn, tn + fp)\n",
    "    accuracy_ci = calculate_wilson_ci(tp + tn, tp + fp + tn + fn)\n",
    "    \n",
    "    # F1 CI using bootstrap approximation\n",
    "    if f1_val > 0 and (tp + fn) > 0:\n",
    "        se_f1 = np.sqrt(f1_val * (1 - f1_val) / (tp + fn))\n",
    "        z = stats.norm.ppf(0.975)\n",
    "        f1_ci_lower = max(0, f1_val - z * se_f1)\n",
    "        f1_ci_upper = min(1, f1_val + z * se_f1)\n",
    "    else:\n",
    "        f1_ci_lower = f1_ci_upper = 0\n",
    "    \n",
    "    per_class_results.append({\n",
    "        'Class': class_list[i],\n",
    "        'TP': int(tp),\n",
    "        'FP': int(fp),\n",
    "        'TN': int(tn),\n",
    "        'FN': int(fn),\n",
    "        'Support': int(support[i]),\n",
    "        'Accuracy': binary_accuracy,\n",
    "        'Accuracy_CI_Lower': accuracy_ci[1],\n",
    "        'Accuracy_CI_Upper': accuracy_ci[2],\n",
    "        'Precision': precision_val,\n",
    "        'Precision_CI_Lower': precision_ci[1],\n",
    "        'Precision_CI_Upper': precision_ci[2],\n",
    "        'Recall': recall_val,\n",
    "        'Recall_CI_Lower': recall_ci[1],\n",
    "        'Recall_CI_Upper': recall_ci[2],\n",
    "        'Specificity': specificity_val,\n",
    "        'Specificity_CI_Lower': specificity_ci[1],\n",
    "        'Specificity_CI_Upper': specificity_ci[2],\n",
    "        'F1_Score': f1_val,\n",
    "        'F1_Score_CI_Lower': f1_ci_lower,\n",
    "        'F1_Score_CI_Upper': f1_ci_upper\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(per_class_results)\n",
    "\n",
    "# Calculate AUC for multiclass (One-vs-Rest approach)\n",
    "n_classes = len(class_list)\n",
    "roc_auc = {}\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "for i in range(n_classes):\n",
    "    # Create binary labels for class i vs rest\n",
    "    y_binary = (test_labels == i).astype(int)\n",
    "    y_scores = test_probabilities[:, i]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_binary, y_scores)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Calculate macro-average ROC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Calculate micro-average ROC\n",
    "y_test_binary = label_binarize(test_labels, classes=range(n_classes))\n",
    "if n_classes == 2:\n",
    "    y_test_binary = np.hstack((1-y_test_binary, y_test_binary))\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binary.ravel(), test_probabilities.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Add AUC to results\n",
    "for i, row in results_df.iterrows():\n",
    "    results_df.loc[i, 'AUC'] = roc_auc[i]\n",
    "\n",
    "print(f\"\\nMacro AUC: {roc_auc['macro']:.4f}, Micro AUC: {roc_auc['micro']:.4f}\")\n",
    "\n",
    "print(\"\\n📋 Per-Class Performance with 95% Confidence Intervals:\")\n",
    "print(\"-\"*160)\n",
    "print(f\"{'Class':>6} | {'Acc':>8} {'[CI]':>12} | {'Prec':>8} {'[CI]':>12} | {'Recall':>8} {'[CI]':>12} | {'Spec':>8} {'[CI]':>12} | {'F1':>8} {'[CI]':>12} | {'AUC':>6} | {'Supp':>4}\")\n",
    "print(\"-\"*160)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['Class']:>6} | \"\n",
    "          f\"{row['Accuracy']:.3f} \"\n",
    "          f\"(±{row['Accuracy_CI_Upper']-row['Accuracy']:.3f}) | \"\n",
    "          f\"{row['Precision']:.3f} \"\n",
    "          f\"(±{row['Precision_CI_Upper']-row['Precision']:.3f}) | \"\n",
    "          f\"{row['Recall']:.3f} \"\n",
    "          f\"(±{row['Recall_CI_Upper']-row['Recall']:.3f}) | \"\n",
    "          f\"{row['Specificity']:.3f} \"\n",
    "          f\"(±{row['Specificity_CI_Upper']-row['Specificity']:.3f}) | \"\n",
    "          f\"{row['F1_Score']:.3f} \"\n",
    "          f\"(±{row['F1_Score_CI_Upper']-row['F1_Score']:.3f}) | \"\n",
    "          f\"{row['AUC']:.3f} | \"\n",
    "          f\"{row['Support']:>4}\")\n",
    "\n",
    "# Add separator line\n",
    "print(\"-\"*160)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = results_df['Accuracy'].mean()\n",
    "avg_specificity = results_df['Specificity'].mean()\n",
    "\n",
    "# Calculate weighted specificity\n",
    "weighted_specificity = np.average(results_df['Specificity'], weights=support)\n",
    "\n",
    "# Add Macro Average row\n",
    "print(f\"{'Macro':>6} | \"\n",
    "      f\"{avg_accuracy:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{macro_precision:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{macro_recall:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{avg_specificity:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{macro_f1:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{roc_auc['macro']:.3f} | \"\n",
    "      f\"{sum(support):>4}\")\n",
    "\n",
    "# Add Weighted Average row\n",
    "print(f\"{'Weight':>6} | \"\n",
    "      f\"{accuracy:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_precision:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_recall:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_specificity:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{weighted_f1:.3f} \"\n",
    "      f\"{'':>12} | \"\n",
    "      f\"{roc_auc['micro']:.3f} | \"\n",
    "      f\"{sum(support):>4}\")\n",
    "\n",
    "# Validation: Check if metrics make sense\n",
    "print(\"\\n🔍 Validation Check:\")\n",
    "print(\"-\"*50)\n",
    "for _, row in results_df.iterrows():\n",
    "    # Check if F1 is reasonable given precision and recall\n",
    "    expected_f1 = 2 * row['Precision'] * row['Recall'] / (row['Precision'] + row['Recall']) if (row['Precision'] + row['Recall']) > 0 else 0\n",
    "    f1_diff = abs(row['F1_Score'] - expected_f1)\n",
    "    \n",
    "    print(f\"{row['Class']:>6}: TP={row['TP']:>3}, FP={row['FP']:>3}, TN={row['TN']:>3}, FN={row['FN']:>3}\")\n",
    "    print(f\"        Expected F1: {expected_f1:.4f}, Actual F1: {row['F1_Score']:.4f}, Diff: {f1_diff:.4f}\")\n",
    "    \n",
    "    # Check sklearn vs manual calculation\n",
    "    sklearn_precision = precision[_] if _ < len(precision) else 0\n",
    "    sklearn_recall = recall[_] if _ < len(recall) else 0\n",
    "    sklearn_f1 = f1[_] if _ < len(f1) else 0\n",
    "    \n",
    "    print(f\"        Sklearn - Prec: {sklearn_precision:.4f}, Recall: {sklearn_recall:.4f}, F1: {sklearn_f1:.4f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Performance metrics bar chart\n",
    "ax1 = axes[0]\n",
    "x_pos = np.arange(len(class_list))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1_Score']\n",
    "colors = ['skyblue', 'orange', 'lightgreen', 'salmon', 'gold']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    values = results_df[metric].values\n",
    "    ci_lower = results_df[f'{metric}_CI_Lower'].values\n",
    "    ci_upper = results_df[f'{metric}_CI_Upper'].values\n",
    "    errors = [values - ci_lower, ci_upper - values]\n",
    "    \n",
    "    bars = ax1.bar(x_pos + i*width - 2*width, values, width, \n",
    "                   yerr=errors, capsize=3, label=metric, \n",
    "                   alpha=0.8, color=color)\n",
    "\n",
    "ax1.set_xlabel('Classes')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Per-Class Performance Metrics with 95% CI')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(class_list)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: Confusion Matrix\n",
    "ax2 = axes[1]\n",
    "im = ax2.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "ax2.set_title('Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(class_list)):\n",
    "    for j in range(len(class_list)):\n",
    "        text = ax2.text(j, i, str(cm[i, j]),\n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "ax2.set_xticks(range(len(class_list)))\n",
    "ax2.set_yticks(range(len(class_list)))\n",
    "ax2.set_xticklabels(class_list)\n",
    "ax2.set_yticklabels(class_list)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary for paper\n",
    "print(\"\\n📝 Summary for Paper:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"The breast cancer classification model achieved an overall accuracy of {accuracy:.4f} on the test set.\")\n",
    "print(f\"The macro-averaged F1-score was {macro_f1:.4f}, indicating {['poor', 'fair', 'good', 'excellent'][min(3, int(macro_f1*4))]} performance across all classes.\")\n",
    "print(f\"The macro-averaged AUC was {roc_auc['macro']:.4f}, demonstrating {['poor', 'fair', 'good', 'excellent'][min(3, int(roc_auc['macro']*4))]} discriminative ability.\")\n",
    "\n",
    "# Individual class performance\n",
    "best_f1_idx = results_df['F1_Score'].idxmax()\n",
    "worst_f1_idx = results_df['F1_Score'].idxmin()\n",
    "print(f\"Best performing class: {results_df.loc[best_f1_idx, 'Class']} (F1-Score: {results_df.loc[best_f1_idx, 'F1_Score']:.4f}, AUC: {results_df.loc[best_f1_idx, 'AUC']:.4f})\")\n",
    "print(f\"Most challenging class: {results_df.loc[worst_f1_idx, 'Class']} (F1-Score: {results_df.loc[worst_f1_idx, 'F1_Score']:.4f}, AUC: {results_df.loc[worst_f1_idx, 'AUC']:.4f})\")\n",
    "\n",
    "# Export results\n",
    "results_summary = {\n",
    "    'Metric': ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1-Score', \n",
    "               'Weighted Precision', 'Weighted Recall', 'Weighted F1-Score', \n",
    "               'Macro AUC', 'Micro AUC'],\n",
    "    'Value': [accuracy, macro_precision, macro_recall, macro_f1, \n",
    "              weighted_precision, weighted_recall, weighted_f1, \n",
    "              roc_auc['macro'], roc_auc['micro']]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "results_df.to_csv(f'{model_path}detailed_per_class_results.csv', index=False)\n",
    "summary_df.to_csv(f'{model_path}overall_results.csv', index=False)\n",
    "cm_df.to_csv(f'{model_path}confusion_matrix.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
